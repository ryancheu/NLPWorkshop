{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Loading\n",
    "\n",
    "Using the quora question duplicates dataset. Loads question pairs into train/test_question_pairs and whether the pair is a duplicate into train/test_question_labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quora_duplicates_train_filename = \"data/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv # for parsing data\n",
    "import matplotlib.pyplot as plt # for plotting results\n",
    "# so that graphs show up in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn # Common machine learning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_question_input_file(filename):\n",
    "    pairs_list = []\n",
    "    labels_list = []\n",
    "    with open(filename) as f:\n",
    "        f.readline() # consume the csv top row which has column names\n",
    "        for row in csv.reader(f):\n",
    "            pairs_list.append((row[3], row[4])) # The two questions for each row\n",
    "            labels_list.append(int(row[5])) # this is whether the questions were marked as duplicates\n",
    "    return pairs_list, labels_list\n",
    "            \n",
    "question_pairs, question_labels = read_question_input_file(quora_duplicates_train_filename)\n",
    "\n",
    "# Shuffle the input together to ensure random split between train and test\n",
    "# If you forget to do this and your data is ordered, you'll often see much higher train than test accuracy\n",
    "question_pairs, question_labels = sklearn.utils.shuffle(question_pairs, question_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into train and test data\n",
    "# Will be used later, initially just working with train\n",
    "train_cutoff = int(len(question_pairs)*0.7)\n",
    "train_question_pairs = question_pairs[:train_cutoff]\n",
    "train_question_labels = question_labels[:train_cutoff]\n",
    "\n",
    "test_question_pairs = question_pairs[train_cutoff:]\n",
    "test_question_labels = question_labels[train_cutoff:]\n",
    "\n",
    "\n",
    "all_train_questions = []\n",
    "all_test_questions = []\n",
    "for pair in train_question_pairs:\n",
    "    all_train_questions.extend([pair[0], pair[1]])\n",
    "for pair in test_question_pairs:\n",
    "    all_test_questions.extend([pair[0], pair[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"num question pairs: \", len(question_pairs))\n",
    "print(\"num train question pairs: \", len(train_question_pairs))\n",
    "print(\"num test question pairs: \", len(test_question_pairs))\n",
    "assert(len(question_pairs) == len(question_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK setup\n",
    "\n",
    "Some Additional datasets are needed for the operations we're using in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "\n",
    "The code below is part of the presentation on text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "`preprocess_question` is a function to perform various text normalization techniques. Primarily, this is done by attempting to reduce the text to the simplest form that still carries all the meaning that we're concerned with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def preprocess_question(question,\n",
    "                        split_method=\"spaces\", # either \"spaces\" or \"tokenization\"\n",
    "                        use_lowercase=False,\n",
    "                        stem_method=None, # None, \"stemming\" or \"lemmatize\"\n",
    "                        use_remove_stopwords=False,\n",
    "                        use_remove_punctuation=False,\n",
    "                        verbose=False):\n",
    "    \"\"\"Takes as input a question text string, and produces a list of tokens.\n",
    "    \n",
    "    split_method: either \"spaces\" or \"tokenization\". Determines method used to split up string.\n",
    "    use_lowercase: if True, will lower case all tokens\n",
    "    stem_method: None, \"stemming\" or \"lemmatize\". Determines method used to find base word.\n",
    "    use_remove_stopwords: If True, will remove stopwords from the question tokens\n",
    "    use_remove_punctuation: If True, will remove all punctuation tokens\n",
    "    verbose: If True, will print the results of each step.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def vprint(*args):\n",
    "        if verbose:\n",
    "            print(args)\n",
    "\n",
    "    vprint(\"input question:\", question)\n",
    "    \n",
    "    def tokenize(question):\n",
    "        # Take the text and break it into words\n",
    "        # Handles punctuation better than text.split()\n",
    "        tokens = nltk.word_tokenize(question)\n",
    "        vprint(\"tokenized:\", tokens)\n",
    "        return tokens\n",
    "    \n",
    "    def basic_split(question):\n",
    "        tokens = question.split(' ')\n",
    "        vprint(\"split:\", tokens)\n",
    "        return tokens\n",
    "\n",
    "    def lowercase(tokens):\n",
    "        tokens = [t.lower() for t in tokens] \n",
    "        return tokens\n",
    "    \n",
    "    def stem(tokens):\n",
    "        # For non-acrynonyms\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "        vprint(\"stemmed:\", tokens)\n",
    "        return tokens\n",
    "    \n",
    "    def lemmatize(tokens):\n",
    "        token_pos_pairs = nltk.pos_tag(tokens)\n",
    "        vprint(\"part of speech tagged: \", token_pos_pairs)\n",
    "        tokens = [lemmatizer.lemmatize(pair[0], get_wordnet_pos(pair[1])) for pair in token_pos_pairs]\n",
    "        vprint(\"lemmatized:\", tokens)\n",
    "        return tokens\n",
    "    \n",
    "    def remove_stopwords(tokens):\n",
    "        tokens = [t for t in tokens if t not in stopwords]\n",
    "        vprint(\"stopwords removed:\", tokens)\n",
    "        return tokens\n",
    "    \n",
    "    def remove_punctuation(tokens):\n",
    "        tokens = [t for t in tokens if t not in string.punctuation]\n",
    "        vprint('punctuation removed', tokens)\n",
    "        return tokens\n",
    "    \n",
    "    if split_method == \"spaces\":\n",
    "        tokens = basic_split(question)\n",
    "    else:\n",
    "        tokens = tokenize(question)\n",
    "    \n",
    "    if use_lowercase:\n",
    "        tokens = lowercase(tokens)\n",
    "    \n",
    "    if stem_method == \"stemming\":\n",
    "        tokens = stem(tokens)\n",
    "    elif stem_method == \"lemmatize\":\n",
    "        tokens = lemmatize(tokens)\n",
    "    \n",
    "    if use_remove_stopwords:\n",
    "        tokens = remove_stopwords(tokens)\n",
    "        \n",
    "    if use_remove_punctuation:\n",
    "        tokens = remove_punctuation(tokens)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "test_tokens = preprocess_question(\n",
    "    \"In tennis, why isn't 30-30 called Deuce? And 40-30 (or 30-40) called Advantage?\",\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting similarty with simple similarity methods\n",
    "\n",
    "Starting with the simple operation of returning whether the number of shared tokens after preprocessing are greater than some threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_predictor(q1, q2, threshold=3, **kwargs):\n",
    "    \"\"\"Takes two questions, returns true if the intersection of their tokens is larger than threshold.\"\"\"\n",
    "    q1_tokens = set(preprocess_question(q1, **kwargs))\n",
    "    q2_tokens = set(preprocess_question(q2, **kwargs))\n",
    "    return len(q1_tokens.intersection(q2_tokens)) >= threshold    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_predictor_binary(predictor, question_pairs, labels, **kwargs):\n",
    "    \"\"\"Given a predictor function and data to evaluate on, calculate recall, precision and accuracy.\"\"\"\n",
    "    y_pred = []\n",
    "    for pair in question_pairs:\n",
    "        y_pred.append(int(predictor(*pair, **kwargs)))\n",
    "    \n",
    "    # Of all labels that should have been predicted positive, how many were?\n",
    "    recall = sklearn.metrics.recall_score(y_true=labels,\n",
    "                                          y_pred=y_pred)\n",
    "    print('recall:', recall)\n",
    "    \n",
    "    # Of the labels that were predicted positive, how many actually should have been?\n",
    "    precision = sklearn.metrics.precision_score(y_true=labels,\n",
    "                                                y_pred=y_pred)\n",
    "    print('precision:', precision) \n",
    "    \n",
    "    # What percent of predictions matched the true label?\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_true=labels,\n",
    "                                              y_pred=y_pred)\n",
    "    print('accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out various configurations of preprocessing and thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 1000 # This takes awhile otherwise, use a smaller value if this takes too long for you\n",
    "eval_pairs = train_question_pairs[:cutoff]\n",
    "eval_labels = train_question_labels[:cutoff]\n",
    "\n",
    "basic_args = (basic_predictor,\n",
    "              eval_pairs,\n",
    "              eval_labels)\n",
    "\n",
    "for threshold in [2,5]:\n",
    "    print('\\nTHRESHOLD', threshold)\n",
    "    eval_predictor_binary(*basic_args,\n",
    "                          threshold=threshold)\n",
    "    print('TOKENIZATION')\n",
    "    eval_predictor_binary(*basic_args,\n",
    "                           threshold=threshold,\n",
    "                           split_method=\"tokenization\")\n",
    "    print('LOWERCASE')\n",
    "    eval_predictor_binary(*basic_args,\n",
    "                           threshold=threshold,\n",
    "                           split_method=\"tokenization\",\n",
    "                           use_lowercase=True)\n",
    "    print('ALL')\n",
    "    eval_predictor_binary(*basic_args,\n",
    "                           threshold=threshold,\n",
    "                           split_method=\"tokenization\",\n",
    "                           use_lowercase=True,\n",
    "                           stem_method=\"lemmatize\",\n",
    "                           use_remove_stopwords=True,\n",
    "                           use_remove_punctuation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline comparison\n",
    "\n",
    "Compare to the predictor that simply returns a random result to get a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def random_predictor(q1, q2, prob_pos=0.5):\n",
    "    return random.random() < prob_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "random_args = (random_predictor, eval_pairs, eval_labels)\n",
    "print('RANDOM BASELINE')\n",
    "eval_predictor_binary(*random_args, prob_pos = 0.5)\n",
    "print('20% positive')\n",
    "eval_predictor_binary(*random_args, prob_pos = 0.2)\n",
    "print('1% positive')\n",
    "eval_predictor_binary(*random_args, prob_pos = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability predictions\n",
    "\n",
    "Instead of using a fixed threshold, output a real number, and then a threshold can be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_prob_predictor(q1, q2, use_jaccard=False, **kwargs):\n",
    "    \"\"\"Basic real valued predictoin using jaccard similarity, or a simple measure of intersection/10.\"\"\"\n",
    "    q1_tokens = set(preprocess_question(q1, **kwargs))\n",
    "    q2_tokens = set(preprocess_question(q2, **kwargs))\n",
    "    q_intersect = len(q1_tokens.intersection(q2_tokens))\n",
    "    if use_jaccard:\n",
    "        q_union = 1 + len(q1_tokens.union(q2_tokens))\n",
    "        return q_intersect/q_union\n",
    "    else:\n",
    "        return q_intersect/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_predictor_prob(q1, q2):\n",
    "    \"\"\"Baaseline random probability comparison\"\"\"\n",
    "    return random.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability predictor evaluation\n",
    "\n",
    "Here, we use pyplot to graphically show how well the classifier predicts.\n",
    "\n",
    "There's a tradeoff between precision and recall. If we simply return that all pairs are duplicates, the recall will be 1.0. On the other hand, if we only mark exact text matches as duplicates, our precision will be 1.0, but our recall will be extremely low. An F1-score is an attempt at balancing the importance of precision and recall, but it is insufficient to look only at the max F1-score to determine the best classifier. Some use cases may care much more about precision than recall, or vise-versa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_predictor_prob(predictor, question_pairs, labels, name=\"\", **kwargs):\n",
    "    pred = []\n",
    "    for pair in question_pairs:\n",
    "        pred.append(predictor(*pair, **kwargs))\n",
    "    \n",
    "    precision, recall, threshold = sklearn.metrics.precision_recall_curve(\n",
    "                                        y_true=labels,\n",
    "                                        probas_pred=pred)\n",
    "    \n",
    "    \n",
    "    # f1_score is the harmonic average between precision and recall\n",
    "    f1_score = [2 * (precision[i] * recall[i]) \n",
    "                / (precision[i] + recall[i]) for i in range(len(precision))]\n",
    "    \n",
    "    fig, plts = plt.subplots(1, 3, figsize=[12,4])\n",
    "    fig.suptitle(name)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    \n",
    "    for subplot in plts:\n",
    "        subplot.grid(True)\n",
    "        subplot.set_ylim(ymin=0, ymax=1)\n",
    "    \n",
    "    plts[0].set(ylabel=\"Recall/Precision\")\n",
    "    plts[0].set(xlabel=\"Threshold\")\n",
    "    plts[0].grid(True)\n",
    "    plts[0].plot(threshold, precision[:-1], 'b', label=\"precision\")\n",
    "    plts[0].plot(threshold, recall[:-1], 'r', label=\"recall\")\n",
    "    plts[0].legend()\n",
    "    \n",
    "    plts[1].set(ylabel=\"Precision\")\n",
    "    plts[1].set(xlabel=\"Recall\")\n",
    "    plts[1].grid(True)\n",
    "    plts[1].plot(recall, precision)\n",
    "    \n",
    "    plts[2].set(ylabel=\"F1 Score\")\n",
    "    plts[2].set(xlabel=\"Threshold\")\n",
    "    plts[2].grid(True)\n",
    "    plts[2].plot(threshold, f1_score[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_predictor_prob(random_predictor_prob, eval_pairs, eval_labels, name=\"Random Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_predictor_prob(basic_prob_predictor, eval_pairs, eval_labels,\n",
    "                    name=\"Space Split Jaccard\",\n",
    "                    use_jaccard=True)\n",
    "eval_predictor_prob(basic_prob_predictor, eval_pairs, eval_labels,\n",
    "                    name=\"All normalization Jaccard\",\n",
    "                    use_jaccard=True,\n",
    "                    split_method=\"tokenization\",\n",
    "                    use_lowercase=True,\n",
    "                    stem_method=\"lemmatize\",\n",
    "                    use_remove_stopwords=True,\n",
    "                    use_remove_punctuation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Grams\n",
    "\n",
    "This is a common technique to take into account some degree of word ordering when using bag of word techniques. Instead of only considering single words (unigrams), we also consider the combination of `n` consecutive tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_word_n_grams(tokens, n_values=[1,2]):\n",
    "    \"\"\"Given a list of tokens and n_values, produce a list of corresponding word n_grams\n",
    "    \n",
    "    For example, if n_values = [1], this will produce only unigrams.\n",
    "    If n_values = [1,2], this will proiduce unigrams and bigrams.\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    for n in n_values:\n",
    "        for i in range(len(tokens) + 1 - n):\n",
    "            ret.append(\" \".join(tokens[i+k] for k in range(n)))\n",
    "    return ret\n",
    "\n",
    "print(produce_word_n_grams(test_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_gram_sim_predictor(q1, q2, n_values=[1,2], **kwargs):\n",
    "    q1_tokens = preprocess_question(q1, **kwargs)\n",
    "    q2_tokens = preprocess_question(q2, **kwargs)\n",
    "    q1_ngrams = set(produce_word_n_grams(q1_tokens, n_values))\n",
    "    q2_ngrams = set(produce_word_n_grams(q2_tokens, n_values))\n",
    "    q_intersect = len(q1_ngrams.intersection(q2_ngrams))\n",
    "    q_union = 1 + len(q1_ngrams.union(q2_ngrams)) # 1 + to handle divide by 0\n",
    "    return q_intersect/q_union # jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_predictor_prob(n_gram_sim_predictor, eval_pairs, eval_labels,\n",
    "                    name=\"n-gram predictor unigram\",\n",
    "                    n_values=[1],\n",
    "                    split_method=\"tokenization\",\n",
    "                    use_lowercase=True,\n",
    "                    stem_method=None,\n",
    "                    use_remove_stopwords=True,\n",
    "                    use_remove_punctuation=True)\n",
    "eval_predictor_prob(n_gram_sim_predictor, eval_pairs, eval_labels,\n",
    "                    name=\"n-gram predictor bigram\",\n",
    "                    n_values=[2],\n",
    "                    split_method=\"tokenization\",\n",
    "                    use_lowercase=True,\n",
    "                    stem_method=None,\n",
    "                    use_remove_stopwords=True,\n",
    "                    use_remove_punctuation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one l2 normalizes the produced vectors\n",
    "tfidf_vectorizer_norm = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(1,1))\n",
    "tfidf_vectorizer_norm.fit(all_train_questions)\n",
    "\n",
    "# this one does not\n",
    "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(1,1), norm=None)\n",
    "tfidf_vectorizer.fit(all_train_questions)\n",
    "\n",
    "# Both do lowercasing, tokenizing, removes punctuation already for us\n",
    "# Can also produce ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf_cos_sim(q1, q2):\n",
    "    \"\"\"Computes the cos similiarity of two \"\"\"\n",
    "    # produces two l2 normalized vectors\n",
    "    tfidf_vectors = tfidf_vectorizer_norm.transform([q1,q2])\n",
    "\n",
    "    # This is a dot product, but same as cos-sim since vectors are normalized\n",
    "    return tfidf_vectors.getrow(0).multiply(tfidf_vectors.getrow(1)).sum()\n",
    "\n",
    "# This function will lowercase, tokenize, remove punct, make ngrams\n",
    "tfidf_analyzer = tfidf_vectorizer.build_analyzer()\n",
    "\n",
    "def get_idf_sum(tokens):\n",
    "    return sum([tfidf_vectorizer.idf_[tfidf_vectorizer.vocabulary_[t]] for t in tokens])\n",
    "\n",
    "def idf_jaccard_sim(q1, q2):\n",
    "    \"\"\"For questions, it doesn't make much sense to use the tf, so just sum idf\"\"\"\n",
    "    q1_tokens = set(tfidf_analyzer(q1))\n",
    "    q2_tokens = set(tfidf_analyzer(q2))\n",
    "    \n",
    "    q_intersect = q1_tokens.intersection(q2_tokens)\n",
    "    q_union = q1_tokens.union(q2_tokens)\n",
    "    \n",
    "    return get_idf_sum(q_intersect)/get_idf_sum(q_union)\n",
    "\n",
    "def idf_constant_sim(q1, q2):\n",
    "    \"\"\"For questions, it doesn't make much sense to use the tf, so just sum idf\"\"\"\n",
    "    q1_tokens = set(tfidf_analyzer(q1))\n",
    "    q2_tokens = set(tfidf_analyzer(q2))\n",
    "    \n",
    "    q_intersect = q1_tokens.intersection(q2_tokens)\n",
    "    \n",
    "    return get_idf_sum(q_intersect)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes a long time to run\n",
    "eval_predictor_prob(idf_jaccard_sim, eval_pairs[:1000], eval_labels[:1000], name=\"jaccard idf\")\n",
    "eval_predictor_prob(tfidf_cos_sim, eval_pairs, eval_labels, name=\"cos sim tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
