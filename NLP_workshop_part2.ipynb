{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this part, we look at using machine learning models\n",
    "\n",
    "Broadly, machine learning models help by figuring out how to weight and combine different measures of the text we have. They can also be used to create better representations/measures of text to be used in later downstream models, as in text embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing from part 1\n",
    "\n",
    "This code is unchanged from part 1, it loads in labeled question duplicates data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quora_duplicates_train_filename = \"data/train.csv\"\n",
    "\n",
    "import csv # for parsing data\n",
    "import matplotlib.pyplot as plt # for plotting results\n",
    "# so that graphs show up in the notebook\n",
    "%matplotlib inline\n",
    "import sklearn # Common machine learning functions\n",
    "import keras # neural networks, used later\n",
    "\n",
    "def read_question_input_file(filename):\n",
    "    pairs_list = []\n",
    "    labels_list = []\n",
    "    with open(filename) as f:\n",
    "        f.readline() # consume the csv top row which has column names\n",
    "        for row in csv.reader(f):\n",
    "            pairs_list.append((row[3], row[4])) # The two questions for each row\n",
    "            labels_list.append(int(row[5])) # this is whether the questions were marked as duplicates\n",
    "    return pairs_list, labels_list\n",
    "            \n",
    "question_pairs, question_labels = read_question_input_file(quora_duplicates_train_filename)\n",
    "\n",
    "# Shuffle the input together to ensure random split between train and test\n",
    "# If you forget to do this and your data is ordered, you'll often see much higher train than test accuracy\n",
    "question_pairs, question_labels = sklearn.utils.shuffle(question_pairs, question_labels)\n",
    "\n",
    "# Split into train and test data\n",
    "# Will be used later, initially just working with train\n",
    "TEST_PERCENT = 0.3\n",
    "train_cutoff = int(len(question_pairs)*(1-TEST_PERCENT))\n",
    "train_question_pairs = question_pairs[:train_cutoff]\n",
    "train_question_labels = question_labels[:train_cutoff]\n",
    "\n",
    "test_question_pairs = question_pairs[train_cutoff:]\n",
    "test_question_labels = question_labels[train_cutoff:]\n",
    "\n",
    "\n",
    "all_train_questions = []\n",
    "all_test_questions = []\n",
    "for pair in train_question_pairs:\n",
    "    all_train_questions.extend([pair[0], pair[1]])\n",
    "for pair in test_question_pairs:\n",
    "    all_test_questions.extend([pair[0], pair[1]])\n",
    "\n",
    "print(\"num question pairs: \", len(question_pairs))\n",
    "print(\"num train question pairs: \", len(train_question_pairs))\n",
    "print(\"num test question pairs: \", len(test_question_pairs))\n",
    "assert(len(question_pairs) == len(question_labels))\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def preprocess_question(question,\n",
    "                        split_method=\"spaces\", # either \"spaces\" or \"tokenization\"\n",
    "                        use_lowercase=False,\n",
    "                        stem_method=None, # None, \"stemming\" or \"lemmatize\"\n",
    "                        use_remove_stopwords=False,\n",
    "                        use_remove_punctuation=False,\n",
    "                        verbose=False):\n",
    "    \"\"\"Takes as input a question text string, and produces a list of tokens.\n",
    "    \n",
    "    split_method: either \"spaces\" or \"tokenization\". Determines method used to split up string.\n",
    "    use_lowercase: if True, will lower case all tokens\n",
    "    stem_method: None, \"stemming\" or \"lemmatize\". Determines method used to find base word.\n",
    "    use_remove_stopwords: If True, will remove stopwords from the question tokens\n",
    "    use_remove_punctuation: If True, will remove all punctuation tokens\n",
    "    verbose: If True, will print the results of each step.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def vprint(*args):\n",
    "        if verbose:\n",
    "            print(args)\n",
    "\n",
    "    vprint(\"input question:\", question)\n",
    "    \n",
    "    def tokenize(question):\n",
    "        # Take the text and break it into words\n",
    "        # Handles punctuation better than text.split()\n",
    "        tokens = nltk.word_tokenize(question)\n",
    "        vprint(\"tokenized:\", tokens)\n",
    "        return tokens\n",
    "    \n",
    "    def basic_split(question):\n",
    "        tokens = question.split(' ')\n",
    "        vprint(\"split:\", tokens)\n",
    "        return tokens\n",
    "\n",
    "    def lowercase(tokens):\n",
    "        tokens = [t.lower() for t in tokens] \n",
    "        return tokens\n",
    "    \n",
    "    def stem(tokens):\n",
    "        # For non-acrynonyms\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "        vprint(\"stemmed:\", tokens)\n",
    "        return tokens\n",
    "    \n",
    "    def lemmatize(tokens):\n",
    "        token_pos_pairs = nltk.pos_tag(tokens)\n",
    "        vprint(\"part of speech tagged: \", token_pos_pairs)\n",
    "        tokens = [lemmatizer.lemmatize(pair[0], get_wordnet_pos(pair[1])) for pair in token_pos_pairs]\n",
    "        vprint(\"lemmatized:\", tokens)\n",
    "        return tokens\n",
    "    \n",
    "    def remove_stopwords(tokens):\n",
    "        tokens = [t for t in tokens if t not in stopwords]\n",
    "        vprint(\"stopwords removed:\", tokens)\n",
    "        return tokens\n",
    "    \n",
    "    def remove_punctuation(tokens):\n",
    "        tokens = [t for t in tokens if t not in string.punctuation]\n",
    "        vprint('punctuation removed', tokens)\n",
    "        return tokens\n",
    "    \n",
    "    if split_method == \"spaces\":\n",
    "        tokens = basic_split(question)\n",
    "    else:\n",
    "        tokens = tokenize(question)\n",
    "    \n",
    "    if use_lowercase:\n",
    "        tokens = lowercase(tokens)\n",
    "    \n",
    "    if stem_method == \"stemming\":\n",
    "        tokens = stem(tokens)\n",
    "    elif stem_method == \"lemmatize\":\n",
    "        tokens = lemmatize(tokens)\n",
    "    \n",
    "    if use_remove_stopwords:\n",
    "        tokens = remove_stopwords(tokens)\n",
    "        \n",
    "    if use_remove_punctuation:\n",
    "        tokens = remove_punctuation(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "In the functions below, we extract data about the questions for use in a machine learning model. This is mostly similar to part1, but instead of picking a single method of text treatement, we try all and let the model decide what's important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QuestionData:\n",
    "    \"\"\"This is some data that will be precomputed for each question so feature functions\n",
    "       don't duplicate work.\"\"\"\n",
    "    def __init__(self, original_text):\n",
    "        self.original_text = original_text\n",
    "        self.basic_split = preprocess_question(self.original_text)\n",
    "        self.tokenized = preprocess_question(self.original_text, split_method=\"tokenize\")\n",
    "        self.stemmed = preprocess_question(self.original_text,\n",
    "                                           split_method=\"tokenize\",\n",
    "                                           stem_method=\"stemming\")\n",
    "        self.without_stopwords = preprocess_question(self.original_text,\n",
    "                                                     split_method=\"tokenize\",\n",
    "                                                     use_remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_word_length_features(X, q1_data, q2_data):\n",
    "    \"\"\"Various representations of the length of questions\"\"\"\n",
    "    basic_diff = abs(len(q1_data.basic_split) - len(q2_data.basic_split))\n",
    "    token_diff = abs(len(q1_data.tokenized) - len(q2_data.tokenized))\n",
    "    max_len_basic = max(len(q1_data.basic_split), len(q2_data.basic_split))\n",
    "    X.append(basic_diff)\n",
    "    X.append(token_diff)\n",
    "    X.append(max_len_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_bag_of_words_features(X, q1_data, q2_data):\n",
    "    basic_intersection = len(set(q1_data.basic_split).intersection(set(q2_data.basic_split)))\n",
    "    basic_union = len(set(q1_data.basic_split).union(q2_data.basic_split))\n",
    "    jaccard_basic = basic_intersection/basic_union\n",
    "    \n",
    "    token_intersection = len(set(q1_data.tokenized).intersection(set(q2_data.tokenized)))\n",
    "    token_union = len(set(q1_data.tokenized).union(q2_data.tokenized))\n",
    "    jaccard_token = token_intersection/token_union\n",
    "    \n",
    "    stemmed_intersection = len(set(q1_data.stemmed).intersection(set(q2_data.stemmed)))\n",
    "    stemmed_union = len(set(q1_data.stemmed).union(q2_data.stemmed))\n",
    "    jaccard_stemmed = stemmed_intersection/stemmed_union\n",
    "    \n",
    "    no_stopwords_intersection = len(set(q1_data.stemmed).intersection(set(q2_data.stemmed)))\n",
    "    no_stopwords_union = len(set(q1_data.stemmed).union(q2_data.stemmed))\n",
    "    jaccard_no_stopwords = no_stopwords_intersection/no_stopwords_union\n",
    "    \n",
    "    X.append(basic_intersection)\n",
    "    X.append(basic_union)\n",
    "    X.append(jaccard_basic)\n",
    "    \n",
    "    X.append(token_intersection)\n",
    "    X.append(token_union)\n",
    "    X.append(jaccard_token)\n",
    "    \n",
    "    X.append(stemmed_intersection)\n",
    "    X.append(stemmed_union)\n",
    "    X.append(jaccard_stemmed)\n",
    "    \n",
    "    X.append(no_stopwords_intersection)\n",
    "    X.append(no_stopwords_union)\n",
    "    X.append(jaccard_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional features, such as n grams could be added as well, but for now, we'll only use these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "def compute_single_basic(pair):\n",
    "    q1 = pair[0]\n",
    "    q2 = pair[1]\n",
    "    this_X = []\n",
    "    data1 = QuestionData(q1)\n",
    "    data2 = QuestionData(q2)\n",
    "           \n",
    "    add_word_length_features(this_X, data1, data2) \n",
    "    add_bag_of_words_features(this_X, data1, data2)\n",
    "    \n",
    "    return this_X\n",
    "\n",
    "def compute_X_train_test(train_questions, test_questions, single_func):\n",
    "    \"\"\"Returns X_train, X_test with computed features\"\"\"\n",
    "    \n",
    "    def compute_X(questions):\n",
    "        X = []\n",
    "        \n",
    "        pool = multiprocessing.Pool()\n",
    "        X = pool.map(single_func, questions)\n",
    "        return X\n",
    "\n",
    "    start_time = time.time()\n",
    "    X_train = compute_X(train_questions)\n",
    "    X_test = compute_X(test_questions)\n",
    "    print(\"Computed features in\", time.time() - start_time, \"seconds\")\n",
    "    return (X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_logistic_classifier(X, y, balance_weights=False):\n",
    "    classifier = sklearn.linear_model.LogisticRegression(\n",
    "        class_weight=\"balanced\" if balance_weights else None)\n",
    "    classifier.fit(X, y)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_train = 10000 # for faster iteration, start with only a small part of all data\n",
    "cutoff_test = int(cutoff_train * TEST_PERCENT)\n",
    "X_train, X_test = compute_X_train_test(train_question_pairs[:cutoff_train],\n",
    "                                       test_question_pairs[:cutoff_test],\n",
    "                                       single_func=compute_single_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = train_logistic_classifier(X_train, train_question_labels[:cutoff_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = classifier.score(X_test, test_question_labels[:cutoff_test])\n",
    "train_accuracy = classifier.score(X_train, train_question_labels[:cutoff_train])\n",
    "\n",
    "print('train accuracy:', train_accuracy)\n",
    "print('test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how much do our features help?\n",
    "X_train_constant = [[1] for i in range(cutoff_train)]\n",
    "X_test_constant = [[1] for i in range(cutoff_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_constant = train_logistic_classifier(X_train_constant, train_question_labels[:cutoff_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = classifier_constant.score(X_test_constant, test_question_labels[:cutoff_test])\n",
    "train_accuracy = classifier_constant.score(X_train_constant, train_question_labels[:cutoff_train])\n",
    "\n",
    "# only moderately better\n",
    "print('train accuracy:', train_accuracy)\n",
    "print('test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, eval_X, labels, name=\"\"):\n",
    "    \n",
    "    if type(classifier) == keras.engine.training.Model:\n",
    "        pred = classifier.predict(np.asarray(eval_X))\n",
    "    else:\n",
    "        pred = [p[1] for p in classifier.predict_proba(eval_X)]\n",
    "    \n",
    "    precision, recall, threshold = sklearn.metrics.precision_recall_curve(\n",
    "                                        y_true=labels,\n",
    "                                        probas_pred=pred)\n",
    "    \n",
    "    # f1_score is the harmonic average between precision and recall\n",
    "    f1_score = [2 * (precision[i] * recall[i]) \n",
    "                / (precision[i] + recall[i]) for i in range(len(precision))]\n",
    "    \n",
    "    fig, plts = plt.subplots(1, 3, figsize=[12,4])\n",
    "    fig.suptitle(name)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    \n",
    "    for subplot in plts:\n",
    "        subplot.grid(True)\n",
    "        subplot.set_ylim(ymin=0, ymax=1)\n",
    "    \n",
    "    plts[0].set(ylabel=\"Recall/Precision\")\n",
    "    plts[0].set(xlabel=\"Threshold\")\n",
    "    plts[0].grid(True)\n",
    "    plts[0].plot(threshold, precision[:-1], 'b', label=\"precision\")\n",
    "    plts[0].plot(threshold, recall[:-1], 'r', label=\"recall\")\n",
    "    plts[0].legend()\n",
    "    \n",
    "    plts[1].set(ylabel=\"Precision\")\n",
    "    plts[1].set(xlabel=\"Recall\")\n",
    "    plts[1].grid(True)\n",
    "    plts[1].plot(recall, precision)\n",
    "    \n",
    "    plts[2].set(ylabel=\"F1 Score\")\n",
    "    plts[2].set(xlabel=\"Threshold\")\n",
    "    plts[2].grid(True)\n",
    "    plts[2].plot(threshold, f1_score[:-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classifier(classifier, X_test, test_question_labels[:cutoff_test], \"with features\")\n",
    "evaluate_classifier(classifier_constant, X_test_constant, test_question_labels[:cutoff_test], \"without features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Text embeddings are an extremely popular technique for modern text based machine learning. First, we start by looking at the famous \"king - man + woman = queen\" result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are the 300-dimension GloVe word embeddings \n",
    "# Higher dimension embeddings generally give better results, but take more computation and memory\n",
    "# If it crashes, use the 50-dimension vectors instead.\n",
    "glove_embeddings = \"data/glove.6B/glove.6B.300d.txt\"\n",
    "\n",
    "# Taken from https://fasttext.cc/docs/en/english-vectors.html\n",
    "import io\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    # n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.asarray(list(map(float, tokens[1:])))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nv(vector):\n",
    "    # Function to l2 normalize vector\n",
    "    return vector/np.linalg.norm(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_dict = load_vectors(glove_embeddings) # This line takes awhile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "composition = nv(glove_dict['king']) - nv(glove_dict['man']) + nv(glove_dict['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('composition dot queen', np.dot(nv(composition), nv(glove_dict['queen'])))\n",
    "print('composition dot woman', np.dot(nv(composition), nv(glove_dict['woman'])))\n",
    "print('queen dot woman', np.dot(nv(glove_dict['woman']), nv(glove_dict['queen'])))\n",
    "print('queen dot castle', np.dot(nv(glove_dict['castle']), nv(glove_dict['queen'])))\n",
    "print('composition dot castle', np.dot(nv(glove_dict['castle']), nv(composition)))\n",
    "print('woman dot castle', np.dot(nv(glove_dict['castle']), nv(glove_dict['woman'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using in a model\n",
    "\n",
    "Next let's train a classifier using word embedding similarity as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_embedding_features(X, *q_datas):\n",
    "    # will discard tokens not found in GloVe\n",
    "    word_embeddings = [[glove_dict[t] for t in q_data.tokenized\n",
    "                                if t in glove_dict] for q_data in q_datas]\n",
    "    if any(len(word_embeds) == 0 for word_embeds in word_embeddings):\n",
    "        X.append(0.0) # invalid\n",
    "        return\n",
    "    \n",
    "    question_embeddings = [nv(np.average(word_embeds, axis=0)) for word_embeds in word_embeddings]\n",
    "    assert(len(question_embeddings[0]) == 300)\n",
    "    cos_sim = np.dot(*question_embeddings)\n",
    "    X.append(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_single_with_embeddings(pair):\n",
    "    q1 = pair[0]\n",
    "    q2 = pair[1]\n",
    "    this_X = []\n",
    "    data1 = QuestionData(q1)\n",
    "    data2 = QuestionData(q2)\n",
    "\n",
    "    #add_word_length_features(this_X, data1, data2) \n",
    "    #add_bag_of_words_features(this_X, data1, data2)\n",
    "    add_embedding_features(this_X, data1, data2)\n",
    "\n",
    "    return this_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_train = 10000 # for faster iteration, start with only a small part of all data\n",
    "cutoff_test = int(cutoff_train * TEST_PERCENT)\n",
    "X_train_emb, X_test_emb = compute_X_train_test(train_question_pairs[:cutoff_train],\n",
    "                                       test_question_pairs[:cutoff_test],\n",
    "                                       single_func=compute_single_with_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_emb = train_logistic_classifier(X_train_emb,\n",
    "                                           train_question_labels[:cutoff_train],\n",
    "                                           balance_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classifier(classifier_emb, X_test_emb, test_question_labels[:cutoff_test], \"with embedding similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "This is a pretty broad topic, won't be able to cover much today. Neural networks are very powerful classifiers that have been the focus of much of the recent developments in machine learning and NLP.\n",
    "\n",
    "Here we look at only a simple neural network with a single hidden layer with 5 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def make_simple_dense_model(num_features):\n",
    "    \"\"\"This is a simple model with 5 hidden units.\"\"\"\n",
    "    features_input = Input(shape=(num_features,), dtype='float32')\n",
    "    \n",
    "    first_dense = Dense(5, activation='relu')(features_input)\n",
    "    output = Dense(1, activation='sigmoid')(first_dense)\n",
    "    \n",
    "    model = Model(inputs=[features_input], outputs=[output])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = make_simple_dense_model(len(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(np.asarray(X_train),\n",
    "          np.asarray(train_question_labels[:cutoff_train]),\n",
    "          epochs=5,\n",
    "          validation_split=0.1) # to detect if any overfitting is occurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classifier(model, X_test, test_question_labels[:cutoff_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
